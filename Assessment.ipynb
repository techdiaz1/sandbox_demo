{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project :1 Linear Regression model: Used Cars Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an automobile company XYZ from USA which aspires to enter the US used car market by setting up their company locally to give competition to their counterparts. They want to understand the factors affecting the pricing of cars in the market, since those may be vastly different from the new car market.\n",
    "       \n",
    "The market for used cars has witnessed a significant surge in demand in recent times, surpassing that of new cars. Consequently, pricing strategies for used vehicles have become crucial for businesses to remain competitive in the market. The value of a used car is influenced by a multitude of factors, such as mileage, model, and year of production. It is imperative for industry players to consider these variables while determining the actual worth of a used car.\n",
    "      \n",
    "First step document that lists the output of your exploratory analysis, any issues, or problems you may see with data that need follow-up, and some basic descriptive analysis that you think highlights important outcomes/findings from the data. Based on your findings, the next level of analysis will be charted out. Build a multiple linear regression model for predicting the price of a used car and provide meaningful inferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset: Used_cars_sales.xlsx**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Dictionary**\n",
    "\n",
    "    Name \t                 Name of the Car on Sell \n",
    "    City \t                 City of the Car on Sell \n",
    "    Years             \t     Year of manufacture \n",
    "    Km_driven  \t             Kms Car travelled \n",
    "    Fuel_Type  \t             Petrol, Diesel, CNG \n",
    "    Transmission  \t         Manual, Automatic \n",
    "    Owner_Type     \t         First, Second, Third \n",
    "    Mileage  \t             Mileage of the Car \n",
    "    Engine \t                 Engine Displacement \n",
    "    Power             \t     Power of Engine \n",
    "    Seats              \t     No. of Seats in Car \n",
    "    Selling_price   \t     Selling price of a Car (Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Guidelines:**\n",
    "\n",
    "1.\tEnsure to follow to Use Id’s provided by UNext for naming file as conventions.\n",
    "2.\tCreate GitHub account and submit the GitHub link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Engineering aspect:  \n",
    "\n",
    "Utilize software engineering aspects while building Machine learning model using modular programming principles to organize your code into reusable functions or classes to enhance readability, maintainability, and collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Instructions \n",
    "\n",
    "- The cells in the Jupyter notebook can be executed any number of times for testing the solution\n",
    "- Refrain from modifying the boilerplate code as it may lead to unexpected behavior \n",
    "- The solution is to be written between the comments `# code starts here` and `# code ends here`\n",
    "- On completing all the questions, the assessment is to be submitted on moodle for evaluation\n",
    "- Before submitting the assessment, there should be `no error` while executing the notebook. If there are any error causing code, please comment it.\n",
    "- The kernel of the Jupyter notebook is to be set as `Python 3 (ipykernel)` if not set already\n",
    "- Include imports as necessary\n",
    "- For each of the task, `Note` section will provide you hints to solve the problem.\n",
    "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Utilize software engineering aspects while building Machine learning model using modular programming principles to organize your code into reusable functions or classes to enhance readability, maintainability, and collaboration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading required libraries and packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Import various libraries and modules used in data analysis, machine learning, and visualization tasks in Python such as `pandas`, `numpy`, `sklearn`, `matplotlib`, `seaborn`, `statsmodels`, `statsmodels.api`, `sklearn.model_selection`, `sklearn.linear_model`, `sklearn.metrics`, `sklearn.preprocessing`. There are 2 ways to import the libraries and modules:\n",
    "* import numpy as np\n",
    "* from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load the 'car_sales-dataset' available in the Dataset folder within the Project folder on the desktop. Perform preliminary EDA with key observations and insights- (weightage - 20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.1: Load the Used_cars_sales dataset using try and except blocks.          (weightage - 2 marks) (AE)        \n",
    "\n",
    "#### NOTE:\n",
    "- The `read_excel` method in Pandas allows you to read Excel files and convert them into a DataFrame, which is a two-dimensional tabular data structure in Pandas.\n",
    "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_the_dataset(dataset_location : str)->pd.DataFrame:\n",
    "#     car_sales = None\n",
    "    # code starts here\n",
    "    try:\n",
    "        cars_sales = pd.read_excel('Used_cars_sales.xlsx')\n",
    "        return car_sales\n",
    "    except:\n",
    "        return \"File not found. Please check the file path.\"\n",
    "    # code ends here\n",
    "    return car_sales       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# store the result of the dataset\n",
    "dataset_location = 'Used_cars_sales.xlsx'\n",
    "car_sales=load_the_dataset(dataset_location)\n",
    "print(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.2: Which brand is selling most?        (weightage - 2 marks)  (AE)             \n",
    "\n",
    "**Hint: output should be the name of the brand only.**\n",
    "\n",
    "#### NOTE:\n",
    "To count each unique value in the 'name' column of the DataFrame car_sales we can use the value_counts() method. It returns a Series where the index contains unique car names, and the values represent the frequency of each name in the DataFrame.\n",
    "To find the name of the car brand with the maximum count we can use the idxmax() method on the series. It returns the name of the most selling brand in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_value_counts(car_sales):\n",
    "    most_selling_brand = None\n",
    "    #code starts here\n",
    "\n",
    "\n",
    "    #code ends here\n",
    "    return most_selling_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_name_value_counts(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.3: Name the top 5 cities which has highest sales. (weightage - 2 marks)               (AE)\n",
    "**Hint: Output should be city name only in the format of list**\n",
    "\n",
    "#### NOTE:\n",
    "To calculate the count of each unique value in the 'City' column of the DataFrame data use the value_counts() method. It will returns a Series where the index contains unique city names, and the values represent the frequency of each city in the DataFrame.\n",
    "Now to select the top 5 cities with the highest sales count use the head() method on the Series. By default, head() returns the first 5 rows of the Series, which in this case, correspond to the cities with the highest sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_value_counts(data):\n",
    "    city_with_max_sales = None\n",
    "    #code starts here\n",
    "    \n",
    "    #code ends here\n",
    "    return city_with_max_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'name', 'count', and 'dtype' information   \n",
    "city_name=get_city_value_counts(car_sales)\n",
    "list(city_name.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.4: Check missing values in the data in terms of percentage using error handling technique and do missing value treatment.  (weightage - 2 marks)       (AE) \n",
    "\n",
    "#### NOTE:\n",
    "- Find the percentage of missing values in the data by dividing the total number of missing values by the total number of rows and multiplying by 100, you - will get the percentage of missing values for each column. \n",
    "Use `isnull().sum()` to calculate the total number of missing values in each column and `shape[0]` to get the total number of rows in the DataFrame.\n",
    "- Do not use `PRINT` statement inside the `Except` Block. Please use `return` statement only within the except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_check(data):\n",
    "    missing_percentage = None\n",
    "    #code starts here\n",
    "\n",
    "\n",
    "    #code ends here\n",
    "    return missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_check(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "For treating the missing value first define the list of columns containing the names of the columns with missing values which will be treated. Iterating over each column name in the columns list fill the missing values in the dataset for that column with the mode value of that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value treatment\n",
    "def missing_value_treatment(car_sales):\n",
    "    # Example: Replace missing values with mode value\n",
    "\n",
    "        \n",
    "    return car_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = missing_value_treatment(car_sales)\n",
    "print(car_sales.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.5: Detect Outliers in the data and do outlier treatment. Plot a boxplot to visualize outliers in the data. (weightage – 2 marks) (ME)       \n",
    "* The extreme values in selling_price due to genuine market conditions, such as luxury properties.Keep the values as it is.\n",
    "\n",
    "#### NOTE:\n",
    "The `sns.boxplot()` function in Seaborn is used to create a box plot visualization, which is a convenient way to visually summarize the distribution of numerical data and identify outliers. Using this detect the outliers in the `car_sales` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a boxplot to visualize outliers in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function `treat_outliers_iqr` to treat outliers using IQR method. Use `median` as treatment method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "The outliers in the dataset can be treated using the Interquartile Range (IQR) method. It replaces the outliers with either the median or the mean, based on the specified treatment method. Calculate the first quartile (Q1), third quartile (Q3), and Interquartile Range (IQR) of the data along with the lower and upper bounds for outliers. Then replace the outliers with the median of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect and treat outliers using IQR method\n",
    "def treat_outliers_iqr(data,treatment='median'):\n",
    "    #code starts here\n",
    "\n",
    "    \n",
    "    #code ends here\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers and treat them with median value\n",
    "numeric_columns=['km_driven']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the treated DataFrame\n",
    "for column in numeric_columns:\n",
    "    car_sales[column] = treat_outliers_iqr(car_sales[column], treatment='median')\n",
    "\n",
    "# Display the treated DataFrame\n",
    "print(car_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualization -boxplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.6: Which variables are significant in predicting the price of a used car? To answer this question, it is important to understand the correlation between the different variables. i.e how much the other features effect the selling price of a used car. (Bivariate analysis) (weightage - 3 marks)               (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "To understand the correlation between different variables , we have to convert the non numeric columns into numeric columns. Clean the data type of the 'max_power', 'engine', and 'mileage' columns in a DataFrame by converting them into numeric types by removing the units in order to perform correlation on them.\n",
    "\n",
    "Removes the following non-numeric suffix using string manipulation functions:\n",
    "* 'bhp' for max_power, \n",
    "* 'CC' for engine, \n",
    "* 'kmpl' or 'km/kg' for mileage\n",
    "\n",
    "Converts the data type of each of the above column to float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to convert data type of Max power, Engine and Mileage into numeric\n",
    "def clean_car_sales_data(df):\n",
    "    #code starts here\n",
    "\n",
    "    #code ends here\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = clean_car_sales_data(car_sales)\n",
    "print(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between numerical variables and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Using Python function `corr` compute the correlation matrix of a DataFrame df containing numeric columns.\n",
    "Ensure that only numeric columns are included in the correlation computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(df):\n",
    "    correlation = None\n",
    "    #code starts here\n",
    "\n",
    "    #code ends here\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = corr(car_sales)\n",
    "corr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.7: Come up with Insights and validate the hypothesis:                         (weightage: 3 marks)  (AE)\n",
    "\n",
    "(i) Is there a statistically significant difference in the average mileage of cars across different fuel types and transmission types?\n",
    "\n",
    "(ii) Is there a statistically significant difference in the selling prices of cars between individual and dealer sellers?\n",
    "\n",
    "(iii)Is there a relationship between the type of fuel and transmission in cars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Is there a statistically significant difference in the average mileage of cars across different fuel types and transmission types?\n",
    "\n",
    "- If yes, set `result1` as \"There is a significant difference in the average mileage of cars across different fuel types and transmission types\"\n",
    "- If no, set `result1` as \"There is no significant difference in the average mileage of cars across different fuel types and transmission types\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Conduct an Analysis of Variance (ANOVA) test to determine if there is a significant difference in the average mileage of cars across different fuel types and transmission types.\n",
    "* Groups the data to create separate groups for each category.\n",
    "* Then perform the ANOVA test using the stats.f_oneway() function from the scipy.stats module.\n",
    "* Compare the obtained p-value with a significance level (alpha)\n",
    "* Based on the comparison, determine whether there is a significant difference in the average mileage of cars across different fuel types and transmission types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def perform_anova(car_data):\n",
    "    result1 = None\n",
    "    # Code starts here\n",
    "\n",
    "    # Perform ANOVA\n",
    "    \n",
    "    # Compare p-value with significance level (e.g., 0.05)\n",
    "    \n",
    "    # Code ends here\n",
    "    \n",
    "    # Return ANOVA results\n",
    "    return result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "perform_anova(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Is there a statistically significant difference in the selling prices of cars between individual and dealer sellers?\n",
    "\n",
    "- If yes,set `result2` as \"There is a statistically significant difference in the selling prices of cars between individual and dealer sellers\"\n",
    "- If no, set `result2` as \"There is no statistically significant difference in the selling prices of cars between individual and dealer sellers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Conducts an independent two-sample t-test to determine if there is a statistically significant difference in the selling prices of cars between individual and dealer sellers.\n",
    "* Filter the data based on the 'seller_type' column to separate the selling prices of cars sold by individual sellers and dealer sellers.\n",
    "* Perform the independent two-sample t-test using the ttest_ind() function from the scipy.stats module. \n",
    "* Compare the obtained p-value with a significance level (alpha)\n",
    "* Based on the comparison,determines whether there is a statistically significant difference in the selling prices of cars between individual and dealer sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "def seller_type_influence_test(car_sales):\n",
    "    result2 = None\n",
    "    # Code starts here\n",
    "    # Filter data based on seller type\n",
    " \n",
    "\n",
    "    # Perform independent t-test\n",
    "    \n",
    "\n",
    "    # Set significance level\n",
    "    \n",
    "\n",
    "    # Interpret the results\n",
    "    \n",
    "    # Code ends here    \n",
    "    return result2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_type_influence_test(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Is there a relationship between the type of fuel and transmission in cars?\n",
    "\n",
    "- If yes,set `result3` as \"There is a statistically significant relationship between the type of fuel and transmission in cars\"\n",
    "- If no, set `result3` as \"There is no statistically significant relationship between the type of fuel and transmission in cars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Conduct a chi-square test of independence to determine if there is a statistically significant relationship between the type of fuel and transmission in cars.\n",
    "* Creates a contingency table of observed frequencies using the pd.crosstab() function. \n",
    "* Perform the chi-square test of independence using the stats.chi2_contingency() function from the scipy.stats module. \n",
    "* Compare the obtained p-value with a significance level (alpha)\n",
    "* Based on the comparison, determine whether there is a statistically significant relationship between the type of fuel and transmission in cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(car_data):\n",
    "    result3 = None\n",
    "    # Create a contingency table of observed frequencies\n",
    "    \n",
    "    # Perform Chi-square test\n",
    "    \n",
    "\n",
    "    # Compare p-value with significance level (e.g., 0.05)\n",
    "    \n",
    "    # Return Chi-square statistic, p-value, and result\n",
    "    return result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "chi_square_test(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.8: Data Transformation:  remove the given variables for model building process (\"name\",\"Postal_code\",\"Sales_ID\"). Apply normalization technique (standard scaler). (weightage - 2 marks)        (AE)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Drop the varaiables \"name\",\"Postal_code\",\"Sales_ID\" from the dataframe using the drop() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the given variables\n",
    "def drop_var1(data):\n",
    "    # Code starts here\n",
    "    \n",
    "    # Code ends here\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = drop_var1(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.9: Handling categorical features: Apply encoding technique to convert categorical variable into numerical. Use try and except blocks.  (weightage - 2 marks)    (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "Transform city variable to 4 categories such as `North_city, South_city, East_city and West_city` according to the below given list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions\n",
    "def city_region(sales_data):\n",
    "    north_cities = ['Dehradun','Mumbai','Jaipur','Indore','Pune','Aurangabad', 'Delhi','Ludhiana', 'kanpur', 'Gangtok', 'Noida']\n",
    "    south_cities = ['Chennai','Mysore', 'Bangalore', 'Nellore', 'Coimbatore','Mangalore','Hyderabad','Vellore','Thrissur']\n",
    "    east_cities = ['Ranchi', 'Kolkata', 'Jamshedpur', 'Patna', 'Bhubaneshwar']\n",
    "    west_cities = ['Ahmedabad','Kochi','Vadodara', 'Surat']\n",
    "\n",
    "    # Transform the City variable to 4 categories\n",
    "\n",
    "\n",
    "    # Print the updated DataFrame\n",
    "    return sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply city_region() in the original data\n",
    "car_sales=city_region(car_sales)\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop the City variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_var2(data):\n",
    "    data.drop(['City'],axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = drop_var2(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding technique: convert other categorical variable into factor variable (0,1,2.. categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Encode categorical columns in a DataFrame using the `LabelEncoder` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_categorical_columns(df):    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it in the original data\n",
    "car_sales=encode_categorical_columns(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert City_region variable to numeric datatype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1.10: Save the cleaned dataset (cleaned_car_sales.xlsx file) by setting the index=False in your GitHub repository for model building process. (This task is for maintaining the version control of datasets) (weightage - 1 mark)      (ME)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Save the DataFrame car_sales to an Excel file named __\"cleaned_car_sales.xlsx\"__ in the `Project` folder on the desktop without including the index column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it in github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Build Multiple Linear regression model for predicting the price of a used car. (weightage - 30 marks)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.1: Load the cleaned dataset and divide it into predictor and target values (X & y) (weightage – 2 marks) (AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned data\n",
    "def load_the_cleaned_dataset(dataset_location):\n",
    "        cleaned_car_sales = None\n",
    "        # Code starts here\n",
    "        \n",
    "        # Code ends here\n",
    "        return cleaned_car_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = 'cleaned_car_sales.xlsx'\n",
    "cleaned_car_sales=load_the_cleaned_dataset(dataset_location)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate independent features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate independent features and target variable\n",
    "def separate_data_and_target(df):\n",
    "    X, y = np.ndarray([]),np.ndarray([])\n",
    "    #code starts here\n",
    "\n",
    "    #code ends here\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent : X, independent : y\n",
    "X, y = separate_data_and_target(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.2: Split the dataset into train and test in the ratio of 80:20 and apply the scaling technique -RobustScaler. (weightage – 4 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "Split the dataset into training and testing. Separate the dependent (data) and independent (target) columns/fields of the dataset and apply the scaling technique -RobustScaler.The requirement is to build a model that performs regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def split_into_train_and_test_normalize_features(X,y):\n",
    "    X_train, X_test,y_train,y_test = np.ndarray([]),np.ndarray([]),np.ndarray([]),np.ndarray([])\n",
    "    # Splitting dataset to train and test sets 80% train and 20% test\n",
    "    \n",
    "\n",
    "        \n",
    "    #Normalizes selected features in a dataset \n",
    "    # Instantiate the RobustScaler\n",
    "    \n",
    "\n",
    "    # Fit the scaler to the training data and transform it\n",
    "     \n",
    "\n",
    "    # Transform the testing data using the same scaler\n",
    "    \n",
    "   \n",
    "    return X_train, X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing \n",
    "X_train, X_test,y_train,y_test=split_into_train_and_test_normalize_features(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.3: Validate the assumptions of Linear regression model- Linearity, multicollinearity (weightage -6 marks)   (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "Use `pairplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the multicollinearity using VIF for all the variables in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Splits the data into training and testing sets using the train_test_split function from scikit-learn. \n",
    "* Define the dependent variable (y1) and independent variables (X1) using the `dmatrices` function from the `patsy` module. \n",
    "* For each independent variable in X1, it calculates the VIF using the variance_inflation_factor function from the statsmodels.stats.outliers_influence module. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated.\n",
    "* Store the calculated VIF values along with the corresponding feature names in a DataFrame `vif` to display the VIF values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "\n",
    "def multicollinearity(data):   \n",
    "    train, test = train_test_split( data,test_size = 0.2,random_state = 1234 )\n",
    "    \n",
    "    # For each X, calculate VIF and save in dataframe\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multicollinearity(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.4: Build a multiple linear regression model on training data using OLS method and sklearn library . (weightage - 15 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Split the dataset into train and test in the ratio of 80:20.\n",
    "* Set the random_state to 1234\n",
    "* Build the OLS model using statsmodels formula API\n",
    "* The function `fit_the_model_ols` should return the OLS Model and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a model using OLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "def fit_the_model_ols(data):    \n",
    "    ols_model = None\n",
    "    train, test = np.ndarray([]),np.ndarray([])\n",
    "    # Code starts here\n",
    "    \n",
    "    # Building the OLS model using statsmodels formula API\n",
    "        \n",
    "    # Return the model and the test data    \n",
    "    return ols_model,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "ols_model, test_data = fit_the_model_ols(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary of the fitted model\n",
    "summary = ols_model.summary()\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear model using sklearn library** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Initializes a linear regression model object.\n",
    "* Fit the model to the training data (X_train and y_train) using the fit() method of the linear regression object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model_on_the_training_data(data,X_train:np.ndarray,y_train:np.ndarray):\n",
    "    regression = None\n",
    "    # Code starts here\n",
    "\n",
    "    # Code ends here \n",
    "    return regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the training data \n",
    "sklearn_model=fit_the_model_on_the_training_data(cleaned_car_sales,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2.5 Model Versioning:    (weightage -3 marks)  (ME)\n",
    "\n",
    "Save the initial version of a linear regression model OLS codebase named 'first_model' to a version \n",
    "control system GitHub using git commands for facilitating collaboration, tracking \n",
    "changes, and ensuring transparency in model development.\n",
    "\n",
    "Save the version of a linear regression model (sklearn) codebase named 'second_model' to a version control system GitHub using git commandsin git bash, if required facilitating collaboration, tracking changes, and ensuring transparency in model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Versioning: \n",
    "\n",
    "Save the above version of a linear regression model (sklearn) codebase named 'second_model' to a version control system GitHub using git commandsin git bash, if required facilitating collaboration, tracking changes, and ensuring transparency in model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Evaluate the performance of the model using the right evaluation metrics. (weightage - 35 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.1: Bring the initial version of a linear regression model 'first_model' and second version ‘second_model’ from a GitHub using git commands and evaluate the model (weightage - 2 marks) (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Versioning:\n",
    "\n",
    "Bring the initial version of a linear regression model 'first_model' from a GitHub using git commands and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring the second version of a linear regression model sklearn 'second_model' from GitHub using git commands, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.2: Evaluate the linear regression model with evaluation metrics R2 and RMSE using OLS and sklearn library. (weightage - 4 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the OLS model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "* Make the predictions on the test set using the provided OLS linear regression model \n",
    "* Calculate the RMSE by comparing the predicted selling prices with the actual selling prices in the test set, using the mean_squared_error function from scikit-learn.\n",
    "* Calculate the R-squared (R²) value using the rsquared attribute of the linear regression model.\n",
    "* Return both `RMSE` and `R²` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def ols_calculate_rmse_r2(model, test_data):\n",
    "    rmse,r_squared = 0.0,0.0\n",
    "    # Making predictions on the test set\n",
    "    \n",
    "\n",
    "    # Calculating RMSE\n",
    "    \n",
    "    \n",
    "    return rmse,r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_rmse, ols_r_squared = ols_calculate_rmse_r2(ols_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Model RMSE value\n",
    "print (ols_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS R2 value\n",
    "print (ols_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Sklearn model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Make the predictions using a trained scikit-learn model (model) on a test dataset (X_test)\n",
    "* Using the trained model predict the target variable for the provided test dataset (X_test) using the predict() method of the model.\n",
    "* Return the predicted target variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_the_sklearn_model(model, X_test: np.ndarray) -> np.ndarray:\n",
    "    y_prediction = np.ndarray([])\n",
    "    # code starts here\n",
    "\n",
    "    # code ends here\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=test_the_sklearn_model(sklearn_model,X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model with evaluation metric R2 and RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "def calculate_r_squared_sklearn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared (coefficient of determination) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        R-squared value.\n",
    "    \"\"\"\n",
    "    r_squared = None\n",
    "    # Code starts here\n",
    "  \n",
    "    # Code ends here\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_sklearn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        RMSE value.\n",
    "    \"\"\"\n",
    "    rmse = None\n",
    "    # Code starts here\n",
    "   \n",
    "    # Code ends here     \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R-squared\n",
    "r_squared = calculate_r_squared_sklearn(y_test, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "rmse = calculate_rmse_sklearn(y_test, y_pred)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.3:Remove the insignificant variables which identified by OLS and multicollinearity -VIF technique. Then apply log transformation on target variable and build the model using sklearn library. Save the version of this linear regression model named 'final_model' to a Github, and evaluate it with R2 and RMSE(weightage - 5 marks) (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove insignificant variable**\n",
    "#### NOTE:\n",
    "* Because of multicollinearity issue drop the variable engine\n",
    "* Because of insignificance drop the variables - City_region, owner, Sales_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_variables(data):\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_car_sales=drop_variables(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Log transformation on target variable**\n",
    "\n",
    "#### NOTE:\n",
    "* Transform the target variable 'selling_price' into its natural logarithm ('ln_selling_price') using the numpy log function\n",
    "* Return the ln_selling_price' column, which contains the transformed target variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_transform(data):\n",
    "   \n",
    "   return data[\"ln_selling_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate independent features and target variable**\n",
    "#### NOTE:\n",
    "* Extract the feature variables (X) by dropping the columns 'ln_selling_price' and 'selling_price' from the DataFrame df using the drop() method\n",
    "* Extract the target variable (y) by selecting only the 'ln_selling_price' column from the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data_and_target_new(df):\n",
    "    X,y = np.ndarray([]),np.ndarray([])\n",
    "    # Code starts here\n",
    "   \n",
    "    # Code ends here\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = separate_data_and_target_new(cleaned_car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing \n",
    "X_train, X_test,y_train,y_test=split_into_train_and_test_normalize_features(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=fit_the_model_on_the_training_data(cleaned_car_sales,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Versioning: \n",
    "\n",
    "Save the above version of a linear regression model (sklearn) codebase named 'final_model' to a version control system GitHub using git commandsin git bash, if required facilitating collaboration, tracking changes, and ensuring transparency in model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Versioning: \n",
    "\n",
    "Bring the above version of a linear regression model skelarn 'final_model' from GitHub using git commands, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model on the testing dataset and return the predicted values for the test dataset\n",
    "def test_the_finalmodel(model1, X_test): \n",
    "    y_prediction = np.ndarray([])\n",
    "    # code starts here\n",
    "   \n",
    "    # code ends here\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred_1=test_the_finalmodel(final_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared_finalmodel(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared (coefficient of determination) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        R-squared value.\n",
    "    \"\"\"\n",
    "    r_squared = None\n",
    "    # Code starts here\n",
    "\n",
    "    # Code ends here \n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_finalmodel(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) metric using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "    y_true : numpy.ndarray\n",
    "        True target values.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        RMSE value.\n",
    "    \"\"\"\n",
    "    rmse = None\n",
    "    # Code starts here\n",
    "\n",
    "\n",
    "    # Code ends here \n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r-squared\n",
    "r_squared_final_model = calculate_r_squared_finalmodel(y_test, y_pred_1)\n",
    "print(r_squared_final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse\n",
    "rmse_final_model = calculate_rmse_finalmodel(y_test, y_pred_1)\n",
    "print(rmse_final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.4 Interpret R2 value and explain how one-unit change in mileage impacts the predicted sales price? (weightage - 3 marks) (ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An R-squared (R2) value of 0.86 indicates that approximately 86% of the variance in the target variable\n",
    "- The model is able to explain a large proportion of the variability in the sales prices of used cars based on the chosen features.\n",
    "- R2 value of 0.86 indicates a strong relationship between the features and the sales prices of used cars, suggesting that the model has good predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficient for the 'km_driven' feature\n",
    "def coefficient_km():\n",
    "    km_driven_coefficient = None\n",
    "    # Code starts here\n",
    " \n",
    "    # Code ends here\n",
    "    return km_driven_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_km()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.5: Ensure residues are random i.e. normally distributed using histplot / QQ plot (weightage - 2 marks) (ME)\n",
    "#### NOTE:\n",
    "* Create a histogram plot of the residuals (the differences between the actual and predicted values of the target variable) using Seaborn's histplot function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(y_test, y_pred):\n",
    "\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_plot(y_test, y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "* Generate a Q-Q plot to visually assess the normality of residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Q-Q plot for residual normality\n",
    "def QQ_plot(y_test, y_pred):\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QQ_plot(y_test,y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.6 Utilize Lime/SHAP libraries, explain the prediction of your regression model and give inferences. (weightage - 4 marks)  (ME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3.7:Implement the unit test case and deploy a model using Flask / Streamlit. (weightage - 15 marks) (ME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Summarize the findings of the analysis and draw conclusions with PPT / PDF. (weightage - 15 marks) (ME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing linear regression with other models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Submission guidelines:\n",
    "\n",
    "* Download the Jupyter notebook in the format of html.\n",
    "* Upload it in the lumen (UNext LMS)\n",
    "* Take a screenshot of T3.7 (Deployment) and upload it in the lumen (UNext LMS) \n",
    "* Summarized PPT/ PDF prepared in Task 4 to be uploaded in the lumen (UNext LMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------- **ASSESSMENT ENDS HERE** ---------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
